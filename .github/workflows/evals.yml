name: AI Coach Evaluations

# Anthropic "Demystifying Evals for AI Agents" Framework Implementation
#
# Priority Levels:
# - P0 (Safety): Must never fail, blocks all deployments
# - P1 (Accuracy): Core functionality, high priority
# - P2 (Quality): User value, medium priority
# - P3 (Experience): Nice to have, lower priority
#
# Metric Selection:
# - Pass@1: Deterministic tasks (VDOT calculations)
# - Pass@k: Tool tasks (RAG retrieval)
# - Pass^k: Customer-facing tasks (AI coaching advice)

on:
  pull_request:
    branches: [main, develop]
    paths:
      - 'backend/app/services/ai_*.py'
      - 'backend/app/knowledge/**'
      - 'backend/app/core/ai_constants.py'
      - 'backend/tests/evals/**'
  push:
    branches: [main]
    paths:
      - 'backend/app/services/ai_*.py'
      - 'backend/app/knowledge/**'
  schedule:
    # Nightly at 3 AM KST (6 PM UTC)
    - cron: '0 18 * * *'
  workflow_dispatch:
    inputs:
      eval_type:
        description: 'Evaluation type to run'
        required: true
        default: 'pr_fast'
        type: choice
        options:
          - pr_fast
          - pr_safety
          - nightly_full
          - nightly_llm
          - release_full
          - release_regression
          - manual_single
      run_llm_graders:
        description: 'Enable real LLM graders (costs API credits)'
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: '3.11'

jobs:
  # ==========================================================================
  # P0 Safety Tests - ALWAYS RUN, BLOCKING
  # ==========================================================================
  p0-safety:
    name: P0 Safety Tests (Blocking)
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: backend/requirements*.txt

      - name: Install dependencies
        working-directory: backend
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-asyncio pytest-cov

      - name: Run P0 Safety Tests
        working-directory: backend
        run: |
          # Run tests for safety-critical tasks with strict pass^3 requirements
          pytest tests/evals/test_graders.py -v --tb=short \
            -k "safety or should_not or injury or pain" \
            --junit-xml=p0-results.xml

      - name: Verify P0 Pass Rate
        working-directory: backend
        run: |
          # P0 tasks must have >95% pass rate
          echo "Checking P0 safety task pass rates..."
          python -c "
          import xml.etree.ElementTree as ET
          tree = ET.parse('p0-results.xml')
          root = tree.getroot()
          testsuite = root.find('testsuite')
          tests = int(testsuite.get('tests', 0))
          failures = int(testsuite.get('failures', 0))
          errors = int(testsuite.get('errors', 0))
          if tests > 0:
              pass_rate = (tests - failures - errors) / tests
              print(f'P0 Pass Rate: {pass_rate:.1%}')
              if pass_rate < 0.95:
                  print('ERROR: P0 pass rate below 95% threshold!')
                  exit(1)
          print('P0 Safety check PASSED')
          "

      - name: Upload P0 Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: p0-safety-results
          path: backend/p0-results.xml
          retention-days: 30

  # ==========================================================================
  # P1 Accuracy Tests - Deterministic, Pass@1
  # ==========================================================================
  p1-accuracy:
    name: P1 Accuracy Tests (VDOT, Pass@1)
    runs-on: ubuntu-latest
    needs: p0-safety

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: backend/requirements*.txt

      - name: Install dependencies
        working-directory: backend
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-asyncio

      - name: Run VDOT Tests (Pass@1)
        working-directory: backend
        run: |
          # VDOT calculations are deterministic - use Pass@1
          pytest tests/evals/test_vdot.py -v -m vdot --tb=short \
            --junit-xml=p1-vdot-results.xml
        continue-on-error: false

      - name: Run Metrics Tests
        working-directory: backend
        run: |
          pytest tests/evals/test_metrics_advanced.py -v --tb=short \
            --junit-xml=p1-metrics-results.xml

      - name: Upload P1 Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: p1-accuracy-results
          path: |
            backend/p1-vdot-results.xml
            backend/p1-metrics-results.xml
          retention-days: 30

  # ==========================================================================
  # Grader Tests
  # ==========================================================================
  grader-tests:
    name: Grader Unit Tests
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: backend/requirements*.txt

      - name: Install dependencies
        working-directory: backend
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-asyncio pytest-cov

      - name: Run grader unit tests
        working-directory: backend
        run: |
          pytest tests/evals/test_graders.py -v --tb=short \
            --cov=tests/evals/graders \
            --cov-report=xml:grader-coverage.xml

      - name: Upload coverage
        uses: codecov/codecov-action@v4
        with:
          file: backend/grader-coverage.xml
          flags: graders
          name: grader-coverage

  # VDOT calculation accuracy tests
  vdot-tests:
    name: VDOT Calculation Tests
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: backend/requirements*.txt

      - name: Install dependencies
        working-directory: backend
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-asyncio

      - name: Run VDOT tests
        working-directory: backend
        run: |
          pytest tests/evals/test_vdot.py -v -m vdot --tb=short
        continue-on-error: false

  # RAG retrieval quality (mock-based)
  rag-tests:
    name: RAG Retrieval Tests
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: backend/requirements*.txt

      - name: Install dependencies
        working-directory: backend
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-asyncio

      - name: Run RAG tests
        working-directory: backend
        run: |
          pytest tests/evals/test_rag.py -v -m rag --tb=short

  # Full evaluation with LLM (manual trigger or main branch)
  full-eval:
    name: Full AI Evaluation
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.run_full_eval == 'true'

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: running_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis:
        image: redis:7
        ports:
          - 6379:6379

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: backend/requirements*.txt

      - name: Install dependencies
        working-directory: backend
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-asyncio pytest-cov

      - name: Run full evaluation suite
        working-directory: backend
        env:
          DATABASE_URL: postgresql+asyncpg://test:test@localhost:5432/running_test
          REDIS_URL: redis://localhost:6379/0
          GOOGLE_AI_API_KEY: ${{ secrets.GOOGLE_AI_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          pytest tests/evals/ -v --tb=short -m "not integration" \
            --cov=app/services \
            --cov-report=xml \
            --cov-report=term-missing

      - name: Upload coverage
        uses: codecov/codecov-action@v4
        with:
          file: backend/coverage.xml
          flags: evals
          name: ai-eval-coverage

  # Regression detection on main
  regression-check:
    name: Regression Check
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    needs: [grader-tests, vdot-tests, rag-tests]

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 2

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: backend/requirements*.txt

      - name: Install dependencies
        working-directory: backend
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-asyncio

      - name: Run regression tests
        working-directory: backend
        run: |
          # Run core eval tests and check for regressions
          pytest tests/evals/test_graders.py tests/evals/test_vdot.py -v \
            --tb=short \
            --junit-xml=eval-results.xml

      - name: Check for regressions
        if: failure()
        run: |
          echo "::error::Evaluation regression detected! Check test results."
          exit 1

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: eval-results
          path: backend/eval-results.xml
          retention-days: 30

  # ==========================================================================
  # Summary Job
  # ==========================================================================
  eval-summary:
    name: Evaluation Summary
    runs-on: ubuntu-latest
    needs: [p0-safety, p1-accuracy, grader-tests, vdot-tests, rag-tests]
    if: always()

    steps:
      - name: Check P0 Safety (Blocking)
        run: |
          if [ "${{ needs.p0-safety.result }}" == "failure" ]; then
            echo "::error::P0 SAFETY TESTS FAILED - DEPLOYMENT BLOCKED"
            exit 1
          fi
          echo "P0 Safety tests passed"

      - name: Check other results
        run: |
          if [ "${{ needs.p1-accuracy.result }}" == "failure" ] || \
             [ "${{ needs.grader-tests.result }}" == "failure" ] || \
             [ "${{ needs.vdot-tests.result }}" == "failure" ] || \
             [ "${{ needs.rag-tests.result }}" == "failure" ]; then
            echo "::warning::Some evaluation jobs failed"
          fi
          echo "Evaluation complete"

      - name: Post summary
        run: |
          echo "## ðŸƒ AI Coach Evaluation Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Based on [Anthropic's Demystifying Evals for AI Agents](https://www.anthropic.com/research/evaluations) framework." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Priority-Based Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Priority | Test Suite | Metric | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|----------|------------|--------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸ”´ P0 | Safety Tests | Pass^k >95% | ${{ needs.p0-safety.result == 'success' && 'âœ… PASSED' || 'âŒ BLOCKED' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸŸ  P1 | Accuracy Tests | Pass@1 >95% | ${{ needs.p1-accuracy.result == 'success' && 'âœ… Passed' || 'âš ï¸ Issues' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸŸ¡ P1 | VDOT Tests | Pass@1 | ${{ needs.vdot-tests.result == 'success' && 'âœ… Passed' || 'âš ï¸ Issues' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸŸ¢ P2 | RAG Tests | Pass@k | ${{ needs.rag-tests.result == 'success' && 'âœ… Passed' || 'âš ï¸ Issues' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| âšª | Grader Tests | Unit | ${{ needs.grader-tests.result == 'success' && 'âœ… Passed' || 'âš ï¸ Issues' }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Metric Selection Strategy" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Use Case | Example |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|----------|---------|" >> $GITHUB_STEP_SUMMARY
          echo "| Pass@1 | Deterministic | VDOT calculations |" >> $GITHUB_STEP_SUMMARY
          echo "| Pass@k | Tools | RAG retrieval |" >> $GITHUB_STEP_SUMMARY
          echo "| Pass^k | Customer-facing | AI coaching advice |" >> $GITHUB_STEP_SUMMARY
